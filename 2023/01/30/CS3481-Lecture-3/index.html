<!DOCTYPE html><html lang="zh-HK"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.8.0" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.8.0" type="image/png" sizes="32x32"><meta name="description" content="Decision Tree                           Classification                           Basic Concept        Classification is the task of assigning objects to one of several predefined categories">
<meta property="og:type" content="article">
<meta property="og:title" content="CS3481-Lecture-3">
<meta property="og:url" content="https://lemonkai.github.io/2023/01/30/CS3481-Lecture-3/index.html">
<meta property="og:site_name" content="KAI Blog">
<meta property="og:description" content="Decision Tree                           Classification                           Basic Concept        Classification is the task of assigning objects to one of several predefined categories">
<meta property="og:locale" content="zh_HK">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230206191130.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230206191433.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230206191548.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230206191823.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230206192015.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230206192103.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230206193327.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230206200339.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230206200622.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230206200904.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230206201122.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230206201949.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230206202159.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230206202253.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230206202643.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230206203543.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230206203847.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230206204623.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230206210207.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230206210427.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230213193929.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230213194049.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230213194158.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230213195035.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230213195351.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230213195415.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230213195938.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230213202853.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230213202914.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230213203615.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230213204053.png">
<meta property="article:published_time" content="2023-01-30T13:17:55.000Z">
<meta property="article:modified_time" content="2023-02-26T14:36:47.301Z">
<meta property="article:author" content="KAI">
<meta property="article:tag" content="CS3481">
<meta property="article:tag" content="Fundamental Data Science">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230206191130.png"><title>CS3481-Lecture-3 | KAI Blog</title><link ref="canonical" href="https://lemonkai.github.io/2023/01/30/CS3481-Lecture-3/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.8.0"><link rel="stylesheet" href="css/custom.css"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":false,"scrollDownIcon":false},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"複製","copySuccess":"複製成功","copyError":"複製失敗"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 6.3.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner header-inner--height header-inner--bgcolor"><nav class="header-nav header-nav--sticky"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首頁</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">歸檔</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">分類</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">標籤</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">CS3481-Lecture-3</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">發表於</span><span class="post-meta-item__value">2023-01-30</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新於</span><span class="post-meta-item__value">2023-02-26</span></span></div></header><div class="post-body">
        <h1 id="Decision-Tree"   >
          <a href="#Decision-Tree" class="heading-link"><i class="fas fa-link"></i></a><a href="#Decision-Tree" class="headerlink" title="Decision Tree"></a>Decision Tree</h1>
      
        <h2 id="Classification"   >
          <a href="#Classification" class="heading-link"><i class="fas fa-link"></i></a><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h2>
      
        <h3 id="Basic-Concept"   >
          <a href="#Basic-Concept" class="heading-link"><i class="fas fa-link"></i></a><a href="#Basic-Concept" class="headerlink" title="Basic Concept"></a>Basic Concept</h3>
      <ul>
<li><p>Classification is the task of assigning objects to one of several predefined categories</p>
<span id="more"></span></li>
<li><p>It is an important problem in many applications</p>
<ul>
<li>Detecting spam email messages based on the message header and content</li>
<li>Categorizing cells as malignant or benign based on the results of MRI scans</li>
<li>Classifying galaxies based on their shapes</li>
</ul>
</li>
<li><p>The input data for a classification task is a collection fo records</p>
</li>
<li><p>Each record, also known as an instance or example, is characterized by a tuple(x,y)</p>
<ul>
<li>x is the attribute set</li>
<li>y is the class label, also known as category or target attribute.</li>
</ul>
</li>
<li><p>The class label is a discrete attribute.<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230206191130.png"></p>
</li>
<li><p>Classification is the task of learning a target function f that maps each attribute set x to one of the predefined class labels y.</p>
</li>
<li><p>The target function is also known as a classification model.</p>
</li>
<li><p>A classification model is useful for the following purposes</p>
<ul>
<li><strong>Descriptive modeling</strong></li>
<li><strong>Predictive modeling</strong></li>
</ul>
</li>
<li><p>A classification technique is a systematic approach to perform classification on an input data set</p>
</li>
<li><p>Example include</p>
<ul>
<li>Decision tree classifiers</li>
<li>Neural networks</li>
<li>Support vector machines</li>
</ul>
</li>
</ul>

        <h3 id="General-Framework-for-Classification"   >
          <a href="#General-Framework-for-Classification" class="heading-link"><i class="fas fa-link"></i></a><a href="#General-Framework-for-Classification" class="headerlink" title="General Framework for Classification"></a>General Framework for Classification</h3>
      <ul>
<li><p>A classifcation technique employs a learning algorithm to identify a model that best fits the relationship between the attribute set and the class label of the input data.</p>
</li>
<li><p>The model generated by a learning algorithm should</p>
<ul>
<li>Fit the input data well and</li>
<li>Correctly predict the class labels of records it has never seen before</li>
</ul>
</li>
<li><p>A key objective of the learning algorithm is to build models with good generalization capability.</p>
</li>
<li><p>First, a training set consisting of records whose class labels are known must be provided.</p>
</li>
<li><p>The training set is used to build a classification model.</p>
</li>
<li><p>This model is subsequently applied to the test set, which consists of records which are different from those in the training set.<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230206191433.png"></p>
</li>
</ul>

        <h3 id="Confusion-matrix"   >
          <a href="#Confusion-matrix" class="heading-link"><i class="fas fa-link"></i></a><a href="#Confusion-matrix" class="headerlink" title="Confusion matrix"></a>Confusion matrix</h3>
      <ul>
<li><p>Evaluation of the performance of the model is based on the counts of correctly and incorrectly predicted test records.</p>
</li>
<li><p>These counts are tabulated in a table known as a confusion matrix</p>
</li>
<li><p>Each entry aij in this table denotes the number of records from class i predicted to be of class j.<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230206191548.png"></p>
</li>
<li><p>The total number of correct predictions made by the model is f11 + f00</p>
</li>
<li><p>The total number of incorrect predictions is f10+f01</p>
</li>
<li><p>The information in a confusion matrix can be summarized with the following two measures</p>
<ul>
<li>Accuracy<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230206191823.png"><br>in binary that is<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230206192015.png"></li>
<li>Error rate<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230206192103.png"></li>
</ul>
</li>
<li><p>Most classification algorithms aim at attaining the highest accuracy, or equivalently, the lowest error rate when applied to the test set.</p>
</li>
</ul>

        <h2 id="Decision-tree-construction"   >
          <a href="#Decision-tree-construction" class="heading-link"><i class="fas fa-link"></i></a><a href="#Decision-tree-construction" class="headerlink" title="Decision tree construction"></a>Decision tree construction</h2>
      
        <h3 id="Concept"   >
          <a href="#Concept" class="heading-link"><i class="fas fa-link"></i></a><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h3>
      <ul>
<li><p>We can solve a classification problem by asking a series of carefully crafted quesions about the attributes of the test record.</p>
</li>
<li><p>Each time we receive an answer, a follow-up question is asked.</p>
</li>
<li><p>This process is continued until we reach a conclusion about the class label of the record.</p>
</li>
<li><p>The series of questions and answers can be organized in the form of a decision tree.</p>
</li>
<li><p>It is a hierarchical structure consisting of nodes and directed edges.</p>
</li>
<li><p>The tree has three types of nodes</p>
<ul>
<li>A <strong>root node</strong> that has no incoming edges.</li>
<li>I<strong>nternal nodes</strong>, each of which has exactly one incoming edge and a number of outgoing edges.</li>
<li><strong>Leaf or terminal nodes</strong>, each of which has exactly one incoming edge and no outgoing edges.</li>
</ul>
</li>
<li><p>In a decision tree, each leaf node is assigned a class label.</p>
</li>
<li><p>The non-terminal nodes, which include the root and other internal nodes, contain attribute test conditions to separate records that have different charaacteristics.</p>
</li>
<li><p>Classifying a test record is straightforward once a decision tree has been constructed.</p>
</li>
<li><p>Starting from the root node, we apply the test condition</p>
</li>
<li><p>We then follow the appropriate branch based on the outcome of the test</p>
</li>
<li><p>This will lead us either to</p>
<ul>
<li>Another internal node, at which a new test condition is applied, or</li>
<li>A leaf node</li>
</ul>
</li>
<li><p>The class label associated with the leaf node is then assigned to the record.<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230206193327.png"></p>
</li>
<li><p>Effient algorithms have been developed tot induce a reasonably accurate, although suboptimal, decision tree in a reasonable amount of time.</p>
</li>
<li><p>These algorithms usually employ a greedy strategy that makes a series of locally optimal decisions about which attribut to use for partitioning the data.</p>
</li>
</ul>

        <h3 id="Hunt’s-Algorithm"   >
          <a href="#Hunt’s-Algorithm" class="heading-link"><i class="fas fa-link"></i></a><a href="#Hunt’s-Algorithm" class="headerlink" title="Hunt’s Algorithm"></a>Hunt’s Algorithm</h3>
      <ul>
<li><p>A decision tree is grown in a recursive fashion by paritioning the training records into successively purer subsets</p>
</li>
<li><p>We suppose </p>
<ul>
<li>Us is the set of training records that are associated with node s.</li>
<li><code>C=&#123;c1,c2,..,ck&#125;</code> is the set of class labels.</li>
</ul>
</li>
<li><p>If all the records in Us belong to the same class ck, then s is a leaf node labeled as ck.</p>
</li>
<li><p>If Us contains records that belong to more than one class,</p>
<ul>
<li>An attribute test condition is selected to parition the records into smaller subsets.</li>
<li>An child node is created for each outcome of the test condition.</li>
<li>The records in Us are distributed to the children based on the outcome.</li>
</ul>
</li>
<li><p>The alogirithm is then recutsively appled to each child node.</p>
</li>
<li><p>For each node, let p(ck) denotes the fraction of training records from class ck</p>
</li>
<li><p>In most cases, the leaf node is assigned to the class that has the majority number of training records.</p>
</li>
<li><p>The fraction p(ck) for a node can also be used to estimate the probability that a record assigned to that node belongs to class k.</p>
</li>
<li><p>Decision trees that are too large are susceptible to a phenomenon known as overfitting.</p>
</li>
<li><p>A tree pruning step can be performed to reduce the size of the decision tree.</p>
</li>
<li><p>Pruning helps by trimming the tree branches in a way that improves the generalization error.</p>
</li>
</ul>

        <h2 id="Attribute-Test"   >
          <a href="#Attribute-Test" class="heading-link"><i class="fas fa-link"></i></a><a href="#Attribute-Test" class="headerlink" title="Attribute Test"></a>Attribute Test</h2>
      <ul>
<li><p>Each recursive step of the treee-growing process must select an attribute test condition to divide the records into smaller subsets.</p>
</li>
<li><p>To implement this step, the algorithm must provide</p>
<ul>
<li>A method for specifying the test condition for different attribute types and</li>
<li>An objective measure for evaluating the goodness of each test condition.</li>
</ul>
</li>
</ul>

        <h3 id="Binary-attribute"   >
          <a href="#Binary-attribute" class="heading-link"><i class="fas fa-link"></i></a><a href="#Binary-attribute" class="headerlink" title="Binary attribute"></a>Binary attribute</h3>
      <ul>
<li>The test condition for a binary attribute generates two possible outcomes<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230206200339.png"></li>
</ul>

        <h3 id="Nominal-attributes"   >
          <a href="#Nominal-attributes" class="heading-link"><i class="fas fa-link"></i></a><a href="#Nominal-attributes" class="headerlink" title="Nominal attributes"></a>Nominal attributes</h3>
      <ul>
<li>A norminal attribute can produce binary or multi-way splits.</li>
<li>Thereare 2^(s-1)-1 ways of creating a binary partition of S attribute values.</li>
<li>For a multi-way split, the number of outcomes depends on the number of distinct values for the corresponding attribute.<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230206200622.png"></li>
</ul>

        <h3 id="Ordinal-attributes"   >
          <a href="#Ordinal-attributes" class="heading-link"><i class="fas fa-link"></i></a><a href="#Ordinal-attributes" class="headerlink" title="Ordinal attributes"></a>Ordinal attributes</h3>
      <ul>
<li>Ordinal attributes can also produce binary or multi-way splits.</li>
<li>Ordinal attributes can be grouped as long as the grouping does not violate the order property of the attribute values.<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230206200904.png"></li>
</ul>

        <h3 id="Continuous-attributes"   >
          <a href="#Continuous-attributes" class="heading-link"><i class="fas fa-link"></i></a><a href="#Continuous-attributes" class="headerlink" title="Continuous attributes"></a>Continuous attributes</h3>
      <ul>
<li>The test condition can be expressed as a comparison test x&lt;&#x3D;T pr x &gt; T</li>
<li>For the binary case<ul>
<li>The decision tree algorithm must consider all possible split positions T, and </li>
<li>Select the one that produces that best partition</li>
</ul>
</li>
<li>For the multi-way split<ul>
<li>The algorithm must consider multiple split positions.<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230206201122.png"></li>
</ul>
</li>
</ul>

        <h2 id="conduction-example"   >
          <a href="#conduction-example" class="heading-link"><i class="fas fa-link"></i></a><a href="#conduction-example" class="headerlink" title="conduction example"></a>conduction example</h2>
      <ul>
<li><p>We consider the problem of using a decision tree to predict the number of participants in a marathon race requiring medical attention.</p>
</li>
<li><p>This number depends on attributes such as </p>
<ul>
<li>Temperature forecase (TEMP)</li>
<li>Humidity forecast (HUMID)</li>
<li>Air pollution forecast (AIR)</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230206201949.png"></p>
<ul>
<li><p>In a decision tree, each internal node represents a particular attribute, e.g. TEMP or AIR</p>
</li>
<li><p>Each possible value of that attribute coresponds to a branch of the tree.</p>
</li>
<li><p>Leaf nodes represent classifications, such as <strong>Large</strong> or <strong>Small</strong> number of participants requiring medical attention.<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230206202159.png"></p>
</li>
<li><p>Suppose AIR is selected as the first attribute.</p>
</li>
<li><p>This partitions the examples as follows.<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230206202253.png"></p>
</li>
<li><p>Since the entries of the set <code>&#123;2,5,7,8,10&#125;</code> all coreespond to the case of a large number of participants requiring medical attention, a leaf node is formed.</p>
</li>
<li><p>On the other hand, for the set <code>&#123;1,3,4,6,9&#125;</code></p>
<ul>
<li><strong>TEMP</strong> is selected as the next attribute to be tested.</li>
<li>This further divides this partition into <code>&#123;4&#125;, &#123;3,6&#125; and &#123;1, 9&#125;</code><br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230206202643.png"></li>
</ul>
</li>
</ul>

        <h1 id="Information-theory"   >
          <a href="#Information-theory" class="heading-link"><i class="fas fa-link"></i></a><a href="#Information-theory" class="headerlink" title="Information theory"></a>Information theory</h1>
      
        <h2 id="Concept-1"   >
          <a href="#Concept-1" class="heading-link"><i class="fas fa-link"></i></a><a href="#Concept-1" class="headerlink" title="Concept"></a>Concept</h2>
      <ul>
<li><p>Each attribute reduces a certain amount of uncertainty in the classification process.</p>
</li>
<li><p>We calcualte the amount of uncertainty reduced by the selection of each attribute.</p>
</li>
<li><p>We then select the attribute that provides the greatest uncertanty reduction.</p>
</li>
<li><p>Information theory provides a mathematical formulation for measuring how much information a message contains.</p>
</li>
<li><p>We consider the case where a message is selected among a set of possible messages and transmitted.</p>
</li>
<li><p>The information content of a message depends on </p>
<ul>
<li>The size of this message set, and </li>
<li>The frequency with which each possible message occurs.</li>
</ul>
</li>
<li><p>The amount of information in a message with occurrence probability p is defined as -log2p.</p>
</li>
<li><p>Suppose we are given</p>
<ul>
<li>a set of messages, <code>C=&#123;c1,c2,...,ck&#125;</code></li>
<li>the occurrence porbality p(ck) of each ck.</li>
</ul>
</li>
<li><p>We define the entropy I as the expected in formation content of a message in C:<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230206203543.png"></p>
</li>
<li><p>The entropy is measured in bits.</p>
</li>
</ul>

        <h2 id="Attribute-selection"   >
          <a href="#Attribute-selection" class="heading-link"><i class="fas fa-link"></i></a><a href="#Attribute-selection" class="headerlink" title="Attribute selection"></a>Attribute selection</h2>
      <ul>
<li><p>We can calcualte the entropy of a set of training examples from the occurrence probabilities of the different classes.</p>
</li>
<li><p>In our example</p>
<ul>
<li>p(<strong>Small</strong>) &#x3D; 2&#x2F;10</li>
<li>p(<strong>Large</strong>) &#x3D; 8&#x2F;10</li>
</ul>
</li>
<li><p>The set of training instances is denoted as U</p>
</li>
<li><p>We can calculate the entropy as follows:<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230206203847.png"></p>
</li>
<li><p>The information gain provided by an attribute is the difference between</p>
<ol>
<li>The degree of uncertainty before including the attribute</li>
<li>The degree of uncertainty after including the attribute.</li>
</ol>
</li>
<li><p>Item 2 above is defined as the weighted average of the entropy values of the child nodes of the attribute.</p>
</li>
<li><p>If we select attribute P, with S values, this will partition U into the subsets {U1,U2,…,Us}.</p>
</li>
<li><p>The average degree of uncertainty after selectin P is<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230206204623.png"></p>
</li>
<li><p>The information gain associated with attribute with attribute P is computed as follows.<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230206210207.png"></p>
</li>
<li><p>If the attibute <strong>AIR</strong> is chosen, the examples are partitioned as follows:</p>
<ul>
<li><code>U1=&#123;1,3,4,6,9&#125;</code></li>
<li><code>U2=&#123;2,5,7,8,10&#125;</code></li>
</ul>
</li>
<li><p>The resulting entropy value is<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230206210427.png"></p>
</li>
<li><p>The information gain can be computed as follows:<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230213193929.png"></p>
</li>
<li><p>For the atttribute TEMP which partitions the examples into <code>U1 = &#123;4,5&#125;</code>,<code>U2=&#123;3,6,7,8&#125;</code> and <code>U3=&#123;1,2,9,10&#125;</code>:<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230213194049.png"></p>
</li>
<li><p>For the attribute HUMID which partitions the examples into <code>U1=&#123;4,5,6,7,9,10&#125;</code> and <code>U2=&#123;1,2,3,8&#125;</code><br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230213194158.png"></p>
</li>
<li><p>The attribute AIR correcsponds to the highest information again.</p>
</li>
<li><p>As a result, this attribute will be selected.</p>
</li>
</ul>

        <h2 id="Continuous-attributes-1"   >
          <a href="#Continuous-attributes-1" class="heading-link"><i class="fas fa-link"></i></a><a href="#Continuous-attributes-1" class="headerlink" title="Continuous attributes"></a>Continuous attributes</h2>
      <ul>
<li><p>If attribute P is continuous with value x, we can apply a binary test.</p>
</li>
<li><p>The outcome of the test depends on a threshold value T.</p>
</li>
<li><p>There are two possible outcomes:</p>
<ul>
<li>x &lt;&#x3D;T</li>
<li>x &gt; T</li>
</ul>
</li>
<li><p>The traning set is then partitioned into 2 subsets U1 and U2.</p>
</li>
<li><p>We apply sorting to values of attribute P to abtain the sequence <code>&#123;x1,x2,...,xm&#125;</code></p>
</li>
<li><p>Any threshold between xr and x(r+1) will divide the set into two subsets</p>
<ul>
<li><code>&#123;x1,x2,...,xr&#125;</code></li>
<li><code>&#123;x(r+1),x(r+2),...,xm&#125;</code></li>
</ul>
</li>
<li><p>There are at most m-1 possible splits.</p>
</li>
<li><p>For r &#x3D; 1,…,m-1 such that x(r) !&#x3D; x(r+1), the corresponding threshold is chosen as Tr&#x3D;(x(r)+x(r+1))&#x2F;2</p>
</li>
<li><p>We can then calculate the information gain for each Tr</p>
<ul>
<li><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230213195035.png"></li>
<li>where I(P,Tr) is a function of Tr</li>
</ul>
</li>
<li><p>The threshold Tr which maximizes gain(P,Tr) is then chosen.</p>
</li>
</ul>

        <h2 id="Impurity-measures"   >
          <a href="#Impurity-measures" class="heading-link"><i class="fas fa-link"></i></a><a href="#Impurity-measures" class="headerlink" title="Impurity measures"></a>Impurity measures</h2>
      <ul>
<li><p>The measures developed for selecting the best split are often based on the degree of impurity of the child nodes.</p>
</li>
<li><p>Besides entropy, other examples of imputrity measures include</p>
<ul>
<li>Gini index<ul>
<li><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230213195351.png"></li>
</ul>
</li>
<li>Classification error rate<ul>
<li><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230213195415.png"></li>
</ul>
</li>
</ul>
</li>
<li><p>In the following figure, we compare the values of the imputrity measures ofr binary classification problems.</p>
</li>
<li><p>p refers to the fraction of records that belong to one of the two classes</p>
</li>
<li><p>All three measures attain their maximum value when p &#x3D; 0.5</p>
</li>
<li><p>Then minimum values of the measures are attained when p equals 0 or 1<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230213195938.png"></p>
</li>
</ul>

        <h2 id="Gain-ratio"   >
          <a href="#Gain-ratio" class="heading-link"><i class="fas fa-link"></i></a><a href="#Gain-ratio" class="headerlink" title="Gain ratio"></a>Gain ratio</h2>
      <ul>
<li><p>Impurity measures such as entropy and Gini indesx tend to favor attributes that have a large number of possible values.</p>
</li>
<li><p>In many cases, a test condition that results in a large number of outcomes may not be desirable</p>
</li>
<li><p>This is because the number of records associated with each partition is too small to enable us to make any reliable predictions.</p>
</li>
<li><p>To solve this problem, we can modify the splitting criterion to take into account the number of possible attribute values.</p>
</li>
<li><p>In the case of informatoin agin, we can use the gain ratio which is defined as follows<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230213202853.png"><br>where<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230213202914.png"></p>
</li>
</ul>

        <h1 id="Oblique-decision-tree"   >
          <a href="#Oblique-decision-tree" class="heading-link"><i class="fas fa-link"></i></a><a href="#Oblique-decision-tree" class="headerlink" title="Oblique decision tree"></a>Oblique decision tree</h1>
      <ul>
<li><p>The test condition described so far involve using only a single attribute at a time</p>
</li>
<li><p>The tree-growing procedure can be viewed as the process of partitioning the attribute sapce into disjoint regions.</p>
</li>
<li><p>The border between two neighboring regins of different classes is known as a decisionboundary.</p>
</li>
<li><p>Since the test condition involves only a single attribute, the decision boundaries are parallel to the coordinate axes.</p>
</li>
<li><p>This limits the expressiveness of the decision tree representation for modeling complex relationships among continuous attributes.<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230213203615.png"></p>
</li>
<li><p>An oblique decision tree allows test conditions that involve more than one attribute </p>
</li>
<li><p>The following figure illustrates a data set that connot be classified effectively by a conventional decision tree</p>
</li>
<li><p>This data set can be easily represented by a single node of an oblique decision tree with the test condition x+y&lt;1</p>
</li>
<li><p>However, finding the optimal test condition for a given node can be computationally expensive.<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230213204053.png"></p>
</li>
</ul>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文結束，感謝您的閱讀 ------</div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://lemonkai.github.io/tags/CS3481/">CS3481</a></span><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://lemonkai.github.io/tags/Fundamental-Data-Science/">Fundamental Data Science</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2023/02/02/CS3103-Lecture-2/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">CS3103-Lecture-2</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2023/01/16/CS3481-Lecture-2/"><span class="paginator-prev__text">CS3481-Lectrue-2</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div><div class="comments" id="comments"><div id="utterances-container"></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目錄</span><span class="sidebar-nav-ov">本站概覽</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Decision-Tree"><span class="toc-number">1.</span> <span class="toc-text">
          Decision Tree</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Classification"><span class="toc-number">1.1.</span> <span class="toc-text">
          Classification</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Basic-Concept"><span class="toc-number">1.1.1.</span> <span class="toc-text">
          Basic Concept</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#General-Framework-for-Classification"><span class="toc-number">1.1.2.</span> <span class="toc-text">
          General Framework for Classification</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Confusion-matrix"><span class="toc-number">1.1.3.</span> <span class="toc-text">
          Confusion matrix</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Decision-tree-construction"><span class="toc-number">1.2.</span> <span class="toc-text">
          Decision tree construction</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Concept"><span class="toc-number">1.2.1.</span> <span class="toc-text">
          Concept</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hunt%E2%80%99s-Algorithm"><span class="toc-number">1.2.2.</span> <span class="toc-text">
          Hunt’s Algorithm</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Attribute-Test"><span class="toc-number">1.3.</span> <span class="toc-text">
          Attribute Test</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Binary-attribute"><span class="toc-number">1.3.1.</span> <span class="toc-text">
          Binary attribute</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Nominal-attributes"><span class="toc-number">1.3.2.</span> <span class="toc-text">
          Nominal attributes</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Ordinal-attributes"><span class="toc-number">1.3.3.</span> <span class="toc-text">
          Ordinal attributes</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Continuous-attributes"><span class="toc-number">1.3.4.</span> <span class="toc-text">
          Continuous attributes</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#conduction-example"><span class="toc-number">1.4.</span> <span class="toc-text">
          conduction example</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Information-theory"><span class="toc-number">2.</span> <span class="toc-text">
          Information theory</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Concept-1"><span class="toc-number">2.1.</span> <span class="toc-text">
          Concept</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Attribute-selection"><span class="toc-number">2.2.</span> <span class="toc-text">
          Attribute selection</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Continuous-attributes-1"><span class="toc-number">2.3.</span> <span class="toc-text">
          Continuous attributes</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Impurity-measures"><span class="toc-number">2.4.</span> <span class="toc-text">
          Impurity measures</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gain-ratio"><span class="toc-number">2.5.</span> <span class="toc-text">
          Gain ratio</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Oblique-decision-tree"><span class="toc-number">3.</span> <span class="toc-text">
          Oblique decision tree</span></a></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/stun-logo.svg" alt="avatar"></div><p class="sidebar-ov-author__text">Seize the Day</p></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">14</div><div class="sidebar-ov-state-item__name">歸檔</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">7</div><div class="sidebar-ov-state-item__name">分類</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">8</div><div class="sidebar-ov-state-item__name">標籤</div></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">你已閱讀了 </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2023</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>KAI</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script>function loadUtterances() {
  var d = document, s = d.createElement('script');
  var container = d.getElementById('utterances-container');

  if (!container) {
    return;
  }
  s.src = 'https://utteranc.es/client.js';
  s.setAttribute('repo', 'LemonKAI/KAIBlogComment');
  s.setAttribute('issue-term', 'title');
  s.setAttribute('label', 'utterances');
  s.setAttribute('theme', 'github-light');
  s.setAttribute('crossorigin', 'anonymous');
  s.setAttribute('async', '');
  if (false) {
    s.setAttribute('data-pjax-rm', '');
  }
  container.append(s);
}

if (false) {
  loadUtterances();
} else {
  window.addEventListener('DOMContentLoaded', loadUtterances, false);
}</script><script src="/js/utils.js?v=2.8.0"></script><script src="/js/stun-boot.js?v=2.8.0"></script><script src="/js/scroll.js?v=2.8.0"></script><script src="/js/header.js?v=2.8.0"></script><script src="/js/sidebar.js?v=2.8.0"></script></body></html>