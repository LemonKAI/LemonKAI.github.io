<!DOCTYPE html><html lang="zh-HK"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.8.0" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.8.0" type="image/png" sizes="32x32"><meta name="description" content="Hierarchical clustering">
<meta property="og:type" content="article">
<meta property="og:title" content="CS3481-Lecture-6">
<meta property="og:url" content="https://lemonkai.github.io/2023/03/20/CS3481-Lecture-6/index.html">
<meta property="og:site_name" content="KAI Blog">
<meta property="og:description" content="Hierarchical clustering">
<meta property="og:locale" content="zh_HK">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230320193119.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230320214336.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230320214402.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230320215025.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230320215259.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230320220247.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230320220415.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230320220431.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230320221018.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230320221119.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230320221138.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230320221448.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230320221930.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230320222330.png">
<meta property="article:published_time" content="2023-03-20T10:43:43.000Z">
<meta property="article:modified_time" content="2023-03-20T14:23:33.851Z">
<meta property="article:author" content="KAI">
<meta property="article:tag" content="CS3481">
<meta property="article:tag" content="Fundamental Data Science">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230320193119.png"><title>CS3481-Lecture-6 | KAI Blog</title><link ref="canonical" href="https://lemonkai.github.io/2023/03/20/CS3481-Lecture-6/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.8.0"><link rel="stylesheet" href="css/custom.css"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":false,"scrollDownIcon":false},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"複製","copySuccess":"複製成功","copyError":"複製失敗"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 6.3.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner header-inner--height header-inner--bgcolor"><nav class="header-nav header-nav--sticky"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首頁</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">歸檔</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">分類</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">標籤</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">CS3481-Lecture-6</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">發表於</span><span class="post-meta-item__value">2023-03-20</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新於</span><span class="post-meta-item__value">2023-03-20</span></span></div></header><div class="post-body">
        <h1 id="Hierarchical-clustering"   >
          <a href="#Hierarchical-clustering" class="heading-link"><i class="fas fa-link"></i></a><a href="#Hierarchical-clustering" class="headerlink" title="Hierarchical clustering"></a>Hierarchical clustering</h1>
      <span id="more"></span>

<ul>
<li><p>A hierarchical clustering is a set of nested clusters that are organized as a tree.</p>
</li>
<li><p>There are two basic approaches for generating a hierarchical clustering</p>
<ul>
<li>Agglomerative</li>
<li>Divisive</li>
</ul>
</li>
<li><p>In agglomerative hierarchical clustering, we start with the points as individual clusters.</p>
</li>
<li><p>At each step, we merge the closest pair of clusters.</p>
</li>
<li><p>This requires defining a notion of cluster distance</p>
</li>
<li><p>In divisive hierarchical clustering, we start with one, all-inclusive cluster</p>
</li>
<li><p>At each step, we split a cluster</p>
</li>
<li><p>This process continues until only singleton clusters of individual points remain</p>
</li>
<li><p>in this case, we need to decide</p>
<ul>
<li>Which cluster to split at each step and</li>
<li>How to do the splitting</li>
</ul>
</li>
<li><p>A hierarchical clustering is often displayed graphically using a tree-like diagram called the dendrogram</p>
</li>
<li><p>The dendrogram displays both</p>
<ul>
<li>the cluster-subcluster relationships and </li>
<li>the order in which the clusters are merged(agglomerative) or split (divisive)</li>
</ul>
</li>
<li><p>For sets of 2-D points, a hierarchical clustering can also be graphically represented using a nested cluster diagram.<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230320193119.png"></p>
</li>
<li><p>The basic agglomerative hierarchical clustering algorithm is summarized as follows</p>
<ul>
<li>Compute the distance matrix</li>
<li>Repeat<ul>
<li>merge the closest two clusters</li>
<li>Update the distance matrix to reflect the distance between the new cluster and the original clusters.</li>
</ul>
</li>
<li>Until only one cluster remains</li>
</ul>
</li>
<li><p>Different definitions of cluster distance leads to different versions of hierarchical clustering</p>
</li>
<li><p>These versions include</p>
<ul>
<li>Single link or MIN</li>
<li>Complete link or MAX</li>
<li>Group average</li>
</ul>
</li>
<li><p>We consider the following set of data points.</p>
</li>
<li><p>The Euclidean distance matrix for these data point is shown in the following slide.<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230320214336.png"><br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230320214402.png"></p>
</li>
</ul>

        <h1 id="Single-Link"   >
          <a href="#Single-Link" class="heading-link"><i class="fas fa-link"></i></a><a href="#Single-Link" class="headerlink" title="Single Link"></a>Single Link</h1>
      <ul>
<li><p>We now consider the single link or MIN version of hierarchical clustering</p>
</li>
<li><p>In this case, the distance between two clusters is defined as the minimum of the distance between any two points, with one of the points in the first cluster, and the other point in the second cluster</p>
</li>
<li><p>This technique is good at handling clusters with non-elliptical sahpes</p>
</li>
<li><p>The following figure shows the result of applying the single link technique to our example data.</p>
</li>
<li><p>The left figure shows the nested clusters as a sequence of nested ellipses.</p>
</li>
<li><p>The numbers associated with the ellipes indicate the order of the clustering.</p>
</li>
<li><p>The right figure shows the same information in the form of a dendrogram</p>
</li>
<li><p>The height at which two clusters are merged in the dendrogram reflects the distance of the two clusters.<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230320215025.png"></p>
</li>
<li><p>For example, we see that the distance between points 3 and 6 is 0.11</p>
</li>
<li><p>That is the height at which they are joined into one cluster in the dendrogram.</p>
</li>
<li><p>As another example, the distance between clusters <code>&#123;3,6&#125;</code>and <code>&#123;2,5&#125;</code> is<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230320215259.png"></p>
</li>
</ul>

        <h1 id="Complete-Link"   >
          <a href="#Complete-Link" class="heading-link"><i class="fas fa-link"></i></a><a href="#Complete-Link" class="headerlink" title="Complete Link"></a>Complete Link</h1>
      <ul>
<li><p>We now consider the complete link or MAX version of hierarchical clustering.</p>
</li>
<li><p>In this case, the distance between two clusters is defined as the maximum of the distance between any two points, with one of the points in the first cluster, and the other point in the second cluster</p>
</li>
<li><p>Complete link tends to produce clusters with globular shapes</p>
</li>
<li><p>The following figure shows the results of applying the complete link to our sample data points.</p>
</li>
<li><p>As with single link, points 3 and 6 are merged first.</p>
</li>
<li><p>Points 2 and 5 are then merged</p>
</li>
<li><p>After that, <code>&#123;3,6&#125;</code> is merged with <code>&#123;4&#125;</code><br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230320220247.png"></p>
</li>
<li><p>this can be explained by the following calculations<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230320220415.png"><br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230320220431.png"></p>
</li>
</ul>

        <h1 id="Group-average"   >
          <a href="#Group-average" class="heading-link"><i class="fas fa-link"></i></a><a href="#Group-average" class="headerlink" title="Group average"></a>Group average</h1>
      <ul>
<li><p>We now consider the group average version of hierarchical clsutering </p>
</li>
<li><p>In this case, the distance between two clusters is defined as the average distance across all pairs of points, with one point in each pair belonging to the first cluster, and the other point of the pair belonging to the second cluster.</p>
</li>
<li><p>This is intermediate approach between the single and complete link approaches.</p>
</li>
<li><p>We consider two clusters Ci and Cj, which are of sizes mi and mj respectively</p>
</li>
<li><p>The distance between the two clusters can be expressed by the following equation<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230320221018.png"></p>
</li>
<li><p>The following figure shows the results of applying the group average to our sample data</p>
</li>
<li><p>The distances between some of the clusters are calculated as follows:<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230320221119.png"><br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230320221138.png"></p>
</li>
<li><p>We observe that <code>d(&#123;3,6,4&#125;,&#123;2,5&#125;)</code> is smaller than <code>d(&#123;3,6,4&#125;,&#123;1&#125;)</code> and <code>d(&#123;2,5&#125;,&#123;1&#125;)</code>.</p>
</li>
<li><p>As a result, <code>&#123;3,6,4&#125;</code> and <code>&#123;2,5&#125;</code> are merged at the fourth stage.</p>
</li>
</ul>

        <h1 id="Key-issues"   >
          <a href="#Key-issues" class="heading-link"><i class="fas fa-link"></i></a><a href="#Key-issues" class="headerlink" title="Key issues"></a>Key issues</h1>
      <ul>
<li>Hierarchical clustering is effective when the underlying application requires the creation of a multi-level structure.</li>
<li>However, they are expensive in terms of their computational and storage requirements.</li>
<li>In addition, once a decision is made to merge two clusters, it cannot be undone at a later time.</li>
</ul>

        <h1 id="DBSCAN"   >
          <a href="#DBSCAN" class="heading-link"><i class="fas fa-link"></i></a><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h1>
      <ul>
<li><p>Density-based clustering locates regions of high density that are separated from one another by regions of low density.</p>
</li>
<li><p>DBSCAN is a simple and effective density-based clustering algorithm.</p>
</li>
<li><p>In DBSCAN, we need to estimate the density for a particular point in the data set.</p>
</li>
<li><p>This is performed by counting the number of points within or at a specified radius, Eps, of that point.</p>
</li>
<li><p>The count includes the point itself.</p>
</li>
<li><p>This technique is illustrated in the following figure.</p>
</li>
<li><p>The number of points within or at a radius of Eps of point A is 7, including A itself.<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230320221448.png"></p>
</li>
<li><p>The density of any point will depend on the specified radius.</p>
</li>
<li><p>Suppose the number of points in the data set is m.</p>
</li>
<li><p>If the radius is large enough, then all points will have a density of m.</p>
</li>
<li><p>If the radius is too small, then all points will have a density of 1.</p>
</li>
<li><p>We need to classify a point as being</p>
<ul>
<li>In the interior of a dense region (a core point).</li>
<li>At the edge of a dense region (a border point)</li>
<li>In a sparsely occupied region (a noise point).</li>
</ul>
</li>
<li><p>The concepts of core, border and noise points are illustrated in the following figure.<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230320221930.png"></p>
</li>
<li><p>Core points are in the interior of a density-based cluster.</p>
</li>
<li><p>A point is a core point if the number of points within or at the boundary of a given neighborhood of the point is greater than or equal to a certain threshold MinPts.</p>
</li>
<li><p>The size of the neighborhood is determined by the distance function and a user-specified distance parameter, Eps.</p>
</li>
<li><p>The threshold MinPts is also a user-specified parameter.</p>
</li>
<li><p>In the figure on slide 30, A is a core point for the indicated radius (Eps) if MinPts&#x3D;7.</p>
</li>
<li><p>A border point is not a core point, but falls within or at the boundary of the neighborhood of a core point.</p>
</li>
<li><p>In the figure on slide 30, B is a border point.</p>
</li>
<li><p>A border point can fall within the neighborhoods of several core points.</p>
</li>
<li><p>A noise point is any point that is neither a core point nor a border point.</p>
</li>
<li><p>In the figure on slide 30, C is a noise point.</p>
</li>
<li><p>While there are points which have not been processed:</p>
</li>
<li><p>For one of these points, check whether it is a corepoint or not.</p>
</li>
<li><p>If it is not a core point</p>
<ul>
<li>Label it as a noise point (This label may change later).</li>
</ul>
</li>
<li><p>If it is a core point, label the point and</p>
<ul>
<li>Form a new cluster Cnew using this point. </li>
<li>Include all points within or at the boundary of its Eps-neighborhood, which have not been previously assigned to a cluster, in Cnew.</li>
<li>Insert all these neighboring points into a queue.</li>
<li>While the queue is not empty, <ul>
<li>Remove the first point from the queue.</li>
<li>If this point is not a core point, label it as a border point.</li>
<li>If this point is a core point, label it and check every point within or at the boundary of its Eps-neighborhood which was not previously assigned to a cluster.  </li>
<li>For each of these unassigned neighboring points,<ul>
<li>Assign the point to Cnew.</li>
<li>Insert the point into the queue.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>The left figure on the next slide shows a sample data set with 3000 2-D points.</p>
</li>
<li><p>The right figure shows the resulting clusters found by DBSCAN.</p>
</li>
<li><p>The core points, border points and noise points are also displayed.<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230320222330.png"></p>
</li>
</ul>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文結束，感謝您的閱讀 ------</div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://lemonkai.github.io/tags/CS3481/">CS3481</a></span><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://lemonkai.github.io/tags/Fundamental-Data-Science/">Fundamental Data Science</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2023/03/23/CS3103-Lecture-7/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">CS3103-Lecture-7</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2023/03/02/CS3103-Lecture-6/"><span class="paginator-prev__text">CS3103-Lecture-6</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div><div class="comments" id="comments"><div id="utterances-container"></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目錄</span><span class="sidebar-nav-ov">本站概覽</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Hierarchical-clustering"><span class="toc-number">1.</span> <span class="toc-text">
          Hierarchical clustering</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Single-Link"><span class="toc-number">2.</span> <span class="toc-text">
          Single Link</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Complete-Link"><span class="toc-number">3.</span> <span class="toc-text">
          Complete Link</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Group-average"><span class="toc-number">4.</span> <span class="toc-text">
          Group average</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Key-issues"><span class="toc-number">5.</span> <span class="toc-text">
          Key issues</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#DBSCAN"><span class="toc-number">6.</span> <span class="toc-text">
          DBSCAN</span></a></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/stun-logo.svg" alt="avatar"></div><p class="sidebar-ov-author__text">Seize the Day</p></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">16</div><div class="sidebar-ov-state-item__name">歸檔</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">7</div><div class="sidebar-ov-state-item__name">分類</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">8</div><div class="sidebar-ov-state-item__name">標籤</div></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">你已閱讀了 </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2023</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>KAI</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script>function loadUtterances() {
  var d = document, s = d.createElement('script');
  var container = d.getElementById('utterances-container');

  if (!container) {
    return;
  }
  s.src = 'https://utteranc.es/client.js';
  s.setAttribute('repo', 'LemonKAI/KAIBlogComment');
  s.setAttribute('issue-term', 'title');
  s.setAttribute('label', 'utterances');
  s.setAttribute('theme', 'github-light');
  s.setAttribute('crossorigin', 'anonymous');
  s.setAttribute('async', '');
  if (false) {
    s.setAttribute('data-pjax-rm', '');
  }
  container.append(s);
}

if (false) {
  loadUtterances();
} else {
  window.addEventListener('DOMContentLoaded', loadUtterances, false);
}</script><script src="/js/utils.js?v=2.8.0"></script><script src="/js/stun-boot.js?v=2.8.0"></script><script src="/js/scroll.js?v=2.8.0"></script><script src="/js/header.js?v=2.8.0"></script><script src="/js/sidebar.js?v=2.8.0"></script></body></html>