<!DOCTYPE html><html lang="zh-HK"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.8.0" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.8.0" type="image/png" sizes="32x32"><meta name="description" content="CS3481-Lecture-4-Classifier Evaluation">
<meta property="og:type" content="article">
<meta property="og:title" content="CS3481-Lecture-4-Classifier Evaluation">
<meta property="og:url" content="https://lemonkai.github.io/2023/02/13/CS3481-Lecture-4/index.html">
<meta property="og:site_name" content="KAI Blog">
<meta property="og:description" content="CS3481-Lecture-4-Classifier Evaluation">
<meta property="og:locale" content="zh_HK">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230213205424.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230213210207.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230220183554.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230220183628.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230220192340.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230220192701.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230220192734.png">
<meta property="article:published_time" content="2023-02-13T12:41:21.000Z">
<meta property="article:modified_time" content="2023-02-26T14:34:47.760Z">
<meta property="article:author" content="KAI">
<meta property="article:tag" content="CS3481">
<meta property="article:tag" content="Fundamental Data Science">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230213205424.png"><title>CS3481-Lecture-4-Classifier Evaluation | KAI Blog</title><link ref="canonical" href="https://lemonkai.github.io/2023/02/13/CS3481-Lecture-4/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.8.0"><link rel="stylesheet" href="css/custom.css"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":false,"scrollDownIcon":false},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"複製","copySuccess":"複製成功","copyError":"複製失敗"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 6.3.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner header-inner--height header-inner--bgcolor"><nav class="header-nav header-nav--sticky"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首頁</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">歸檔</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">分類</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">標籤</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">CS3481-Lecture-4-Classifier Evaluation</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">發表於</span><span class="post-meta-item__value">2023-02-13</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新於</span><span class="post-meta-item__value">2023-02-26</span></span></div></header><div class="post-body"><p><strong>CS3481-Lecture-4-Classifier Evaluation</strong></p>
<span id="more"></span>

        <h1 id="Classification-error"   >
          <a href="#Classification-error" class="heading-link"><i class="fas fa-link"></i></a><a href="#Classification-error" class="headerlink" title="Classification error"></a>Classification error</h1>
      <ul>
<li><p>The errors committed by a classification model are generally divided into two types</p>
<ul>
<li>Training errors</li>
<li>Generalization errors</li>
</ul>
</li>
<li><p><strong>Training error</strong> is the number of misclassification errors committed on training records</p>
</li>
<li><p><strong>Training error</strong> is also known as resubstitution error of apparent error.</p>
</li>
<li><p><strong>Generalization error</strong> is the expected error of the model on previously unseen records.</p>
</li>
<li><p>A good classification model should</p>
<ul>
<li>Fit the training data well.</li>
<li>Accurately classify reocrds it has never seen before.</li>
</ul>
</li>
<li><p>A model that fits the training data too well can have a poor generaliztion error.</p>
</li>
<li><p>This is known as model overfitting</p>
</li>
<li><p>We consider the 2-D data set in the following figure.</p>
</li>
<li><p>The data set contains data points that belong to two different calsses.</p>
</li>
<li><p>10% of the points are chosen for training, while the remaining 90% are used for testing.</p>
</li>
<li><p>Decision tree calssifiers with different numbers of leaf nodes are built using the training set.<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230213205424.png"></p>
</li>
<li><p>The following figure shows the training and test error rates of the decision tree.</p>
</li>
<li><p>Both error rates are large when the size of the tree is very small.</p>
</li>
<li><p>This situation is known as model underfitting. </p>
</li>
<li><p>Underfitting occurs because the model cannot learn the true structure of the data.</p>
</li>
<li><p>It performs poorly on both the training and test sets.<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230213210207.png"></p>
</li>
<li><p>When the tree becomes too large</p>
<ul>
<li>The training error rate continues to decrease</li>
<li>However, the test error rate begins to increase.</li>
</ul>
</li>
<li><p>This pheonmenon is known as model overfitting</p>
</li>
</ul>

        <h1 id="Overfitting"   >
          <a href="#Overfitting" class="heading-link"><i class="fas fa-link"></i></a><a href="#Overfitting" class="headerlink" title="Overfitting"></a>Overfitting</h1>
      <ul>
<li><p>The training error can be reduced by increasing the model complexity</p>
</li>
<li><p>However, the test error can be large because the model may accidentally fit some of the noise point in the training data.</p>
</li>
<li><p>In ohter words, the performance of the model on the training set does not generalize well to the test examples.</p>
</li>
<li><p>We consider two decision treews, one with 5 leaf nodes and one with 50 leaf nodes.</p>
</li>
<li><p>The two decision trees and their corresponding decision boundaries are shown in the following figures.<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230220183554.png"><br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230220183628.png"></p>
</li>
<li><p>The decision boundaries constructed by the tree with 5 leaf nodes represent a reasonable approximation of the optimal solution.</p>
</li>
<li><p>It is expected that this set of decision boudnaries will result in a good classification performance on the test set.</p>
</li>
<li><p>On the other hand, the tree with 50 leaf nodes generates a set of complicated decision boundaries.</p>
</li>
<li><p>Some of the shaded regions corresponding to the ‘+’ class only contain a few ‘+’ training instances, among a large number of surrounding ‘o’ instance.</p>
</li>
<li><p>This excessive fine-tuning to specific patterns in the data will lead to unsatisfactory classification performance on the test set.</p>
</li>
</ul>

        <h1 id="Generalization-error-estimation"   >
          <a href="#Generalization-error-estimation" class="heading-link"><i class="fas fa-link"></i></a><a href="#Generalization-error-estimation" class="headerlink" title="Generalization error estimation"></a>Generalization error estimation</h1>
      <ul>
<li><p>The ideal classification model is the one that produeces the lowest generalization error.</p>
</li>
<li><p>The problem is that the model has no knowledge of the test set.</p>
</li>
<li><p>It has access only to the training set.</p>
</li>
<li><p>We consider the following approaches to estimate the generalization error</p>
<ul>
<li>Resubstitution estimate</li>
<li>Estimates incorporating model complexity</li>
<li>Using a validation set</li>
</ul>
</li>
</ul>

        <h2 id="Resubstitution-estimate"   >
          <a href="#Resubstitution-estimate" class="heading-link"><i class="fas fa-link"></i></a><a href="#Resubstitution-estimate" class="headerlink" title="Resubstitution estimate"></a>Resubstitution estimate</h2>
      <ul>
<li><p>The resubstitution estimate approach assumes that the training set is a good representation of the overall data</p>
</li>
<li><p>However, the training error is usually an optimistic estimate of the generalization error.</p>
</li>
<li><p>We consider the two decision trees shown in the following figure.</p>
</li>
<li><p>The left tree TL is more complex than the right tree TR.</p>
</li>
<li><p>The training error rate for TL is e(TL) &#x3D; 4&#x2F;24 &#x3D; 0.167</p>
</li>
<li><p>The training error rate for TR is e(TR) &#x3D; 6&#x2F;25 &#x3D; 0.24</p>
</li>
<li><p>Based on the resubsitution estimate, TL is considered better than TR.<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230220192340.png"></p>
</li>
</ul>

        <h2 id="Estimates-incorporating-model-complexity"   >
          <a href="#Estimates-incorporating-model-complexity" class="heading-link"><i class="fas fa-link"></i></a><a href="#Estimates-incorporating-model-complexity" class="headerlink" title="Estimates incorporating model complexity"></a>Estimates incorporating model complexity</h2>
      <ul>
<li><p>The chance for model overfitting increases as the model becomes more complex.</p>
</li>
<li><p>As a result, we should prefer simpler models</p>
</li>
<li><p>Based on this principle, we can estimate the generalization error as the sum of </p>
<ul>
<li>Training error and</li>
<li>A penalty term for model complexity</li>
</ul>
</li>
<li><p>We consider the previous two decision trees TL and TR</p>
</li>
<li><p>We assume that the penalty term is equal to 0.5 for each leaf node.</p>
</li>
<li><p>The error rate estimate for TL is:<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230220192701.png"></p>
</li>
<li><p>The error rate estimate for TR is:<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230220192734.png"></p>
</li>
<li><p>Based on this penalty term, TL is better than TR</p>
</li>
<li><p>For a binary tree, a penalty term of 0.5 means that a node should always be expanded into its two child nodes if it improves the classification of at least one training record.</p>
</li>
<li><p>This is bcecause expanding a node, which is the same as adding 0.5 to the overall error, is less costly than committing one training error.</p>
</li>
<li><p>Suppose the penalty term is equal to 1 for all the leaf nodes</p>
</li>
<li><p>The error rate estimate for TL becomes 0.458</p>
</li>
<li><p>The error rate estimate for TR becomes 0.417</p>
</li>
<li><p>So TR is better than TL</p>
</li>
<li><p>A penalty term of 1 measns that, for a binary tree, a node should not be expanded unless it reduces the classification error by more than one training record.</p>
</li>
</ul>

        <h2 id="Using-a-validation-set"   >
          <a href="#Using-a-validation-set" class="heading-link"><i class="fas fa-link"></i></a><a href="#Using-a-validation-set" class="headerlink" title="Using a validation set"></a>Using a validation set</h2>
      <ul>
<li>In this approach, the original training data is divided into two smaller subsets.</li>
<li>One of the subsets is used for training</li>
<li>Another one known as the validation set, is used for estimating the generalization error.</li>
<li>This approach can be used in the case where the complexity of the model is determind by a parameter</li>
<li>We can adjust the parameter until the resulting model attains the lowest error on the validation set.</li>
<li>This approach provides a better way for estimating how well the model performs on previously unseen records.</li>
<li>However, less data are available for training</li>
</ul>

        <h1 id="Handling-overfitting-in-decision-tree"   >
          <a href="#Handling-overfitting-in-decision-tree" class="heading-link"><i class="fas fa-link"></i></a><a href="#Handling-overfitting-in-decision-tree" class="headerlink" title="Handling overfitting in decision tree"></a>Handling overfitting in decision tree</h1>
      <ul>
<li>There are two approaches for avoiding model overfitting in decision tree<ul>
<li>Pre-pruning</li>
<li>Post-pruning</li>
</ul>
</li>
</ul>

        <h2 id="Pre-pruning"   >
          <a href="#Pre-pruning" class="heading-link"><i class="fas fa-link"></i></a><a href="#Pre-pruning" class="headerlink" title="Pre-pruning"></a>Pre-pruning</h2>
      <ul>
<li>In this approach, the tree growing algorithm is halted before generating a fully grown tree that perfectly fits the training data.</li>
<li>To do this, an alternative stopping condition could be used</li>
<li>For example, we can stop expanding a node when the observed change in impurity measure falls below a certain threshold.</li>
<li>Advantage:<ul>
<li>avoids generating overly compelx sub-trees that overfit the training data</li>
</ul>
</li>
<li>However, it is difficult to choose the right threshold for the change in impurity measure.</li>
<li>A threshold which is:<ul>
<li>too high will result in underfitted models</li>
<li>too low may not be sufficient to overcome the model overfitting problem</li>
</ul>
</li>
</ul>

        <h2 id="Post-pruning"   >
          <a href="#Post-pruning" class="heading-link"><i class="fas fa-link"></i></a><a href="#Post-pruning" class="headerlink" title="Post-pruning"></a>Post-pruning</h2>
      <ul>
<li>In this approach, the decision tree is initially grown to its maximum size</li>
<li>This is followed by a tree pruning step, which trims the fully grown tree.</li>
<li>Trimming can be done by replacing a sub-tree with a new leaf node whose class label is determined from the majority class of records associated with the node</li>
<li>The tree pruning step terminates when no further improvement is observed</li>
<li>Post-pruning tends to give better results than pre-pruning because it makes pruning decisions based on a fully grown tree.</li>
<li>On the other hand, pre-pruning can suffer from prematrue termination of the tree growing process.</li>
<li>However, for post-pruning, the additional computations for growing the full tree may be wasted when some of the sub-trees are pruned.</li>
</ul>

        <h1 id="Classifier-evaluation"   >
          <a href="#Classifier-evaluation" class="heading-link"><i class="fas fa-link"></i></a><a href="#Classifier-evaluation" class="headerlink" title="Classifier evaluation"></a>Classifier evaluation</h1>
      <ul>
<li>There are a number of methods to evaluate the performance of a classifier<ul>
<li>Hold-out method</li>
<li>Cross validation</li>
</ul>
</li>
</ul>

        <h2 id="Hold-out-method"   >
          <a href="#Hold-out-method" class="heading-link"><i class="fas fa-link"></i></a><a href="#Hold-out-method" class="headerlink" title="Hold-out method"></a>Hold-out method</h2>
      <ul>
<li>In this method, the original data set is partitioned into two disjoint sets.</li>
<li>These are called the training set and test set respectively.</li>
<li>The classification model is constructed from the training set.</li>
<li>The performance of the model is evaluted using the test set.</li>
<li>The hold-out method has a number of well known limitations.</li>
<li>First, fewer examples are available for training.</li>
<li>Second, the model may be highly dependent on the composition of the training and test sets.</li>
</ul>

        <h2 id="Cross-validation"   >
          <a href="#Cross-validation" class="heading-link"><i class="fas fa-link"></i></a><a href="#Cross-validation" class="headerlink" title="Cross validation"></a>Cross validation</h2>
      <ul>
<li>In this approach, each record is used the same number of times ofr training, and exactly once for testing.</li>
<li>To illustrate this method, suppose we partition the data into two equal-sized subsets.</li>
<li>Frist, we choose one of the subsets for training and the other for testing</li>
<li>We then swap the roles of the subsets so that the previous training set becomes the test set, and vice versa.</li>
<li>The estimated error is obtained by averaging the errors on the test sets for both runs.</li>
<li>In this example, eachrecord is used exactly once for training and once for testing.</li>
<li>This approach is called two-fold cross-validation</li>
<li>The k-fold cross validation method generalizes this approach by segmenting the data into k equal-sized partitions.</li>
<li>During each run<ul>
<li>One for the partitions is chosen for testing</li>
<li>The rest of them are used for training</li>
</ul>
</li>
<li>This procedure is repeated k times so that each partition is used for testing exactly once.</li>
<li>The estimated error is obtained by averaging the errors on the test sets for all k runs.</li>
</ul>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文結束，感謝您的閱讀 ------</div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://lemonkai.github.io/tags/CS3481/">CS3481</a></span><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://lemonkai.github.io/tags/Fundamental-Data-Science/">Fundamental Data Science</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2023/02/16/CS3103-Lecture-5/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">CS3103-Lecture-5</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2023/02/09/CS3103-Lecture-4/"><span class="paginator-prev__text">CS3103-Lecture-4</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div><div class="comments" id="comments"><div id="utterances-container"></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目錄</span><span class="sidebar-nav-ov">本站概覽</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Classification-error"><span class="toc-number">1.</span> <span class="toc-text">
          Classification error</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Overfitting"><span class="toc-number">2.</span> <span class="toc-text">
          Overfitting</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Generalization-error-estimation"><span class="toc-number">3.</span> <span class="toc-text">
          Generalization error estimation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Resubstitution-estimate"><span class="toc-number">3.1.</span> <span class="toc-text">
          Resubstitution estimate</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Estimates-incorporating-model-complexity"><span class="toc-number">3.2.</span> <span class="toc-text">
          Estimates incorporating model complexity</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Using-a-validation-set"><span class="toc-number">3.3.</span> <span class="toc-text">
          Using a validation set</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Handling-overfitting-in-decision-tree"><span class="toc-number">4.</span> <span class="toc-text">
          Handling overfitting in decision tree</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Pre-pruning"><span class="toc-number">4.1.</span> <span class="toc-text">
          Pre-pruning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Post-pruning"><span class="toc-number">4.2.</span> <span class="toc-text">
          Post-pruning</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Classifier-evaluation"><span class="toc-number">5.</span> <span class="toc-text">
          Classifier evaluation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Hold-out-method"><span class="toc-number">5.1.</span> <span class="toc-text">
          Hold-out method</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cross-validation"><span class="toc-number">5.2.</span> <span class="toc-text">
          Cross validation</span></a></li></ol></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/stun-logo.svg" alt="avatar"></div><p class="sidebar-ov-author__text">Seize the Day</p></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">10</div><div class="sidebar-ov-state-item__name">歸檔</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">7</div><div class="sidebar-ov-state-item__name">分類</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">8</div><div class="sidebar-ov-state-item__name">標籤</div></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">你已閱讀了 </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2023</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>KAI</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script>function loadUtterances() {
  var d = document, s = d.createElement('script');
  var container = d.getElementById('utterances-container');

  if (!container) {
    return;
  }
  s.src = 'https://utteranc.es/client.js';
  s.setAttribute('repo', 'LemonKAI/KAIBlogComment');
  s.setAttribute('issue-term', 'title');
  s.setAttribute('label', 'utterances');
  s.setAttribute('theme', 'github-light');
  s.setAttribute('crossorigin', 'anonymous');
  s.setAttribute('async', '');
  if (false) {
    s.setAttribute('data-pjax-rm', '');
  }
  container.append(s);
}

if (false) {
  loadUtterances();
} else {
  window.addEventListener('DOMContentLoaded', loadUtterances, false);
}</script><script src="/js/utils.js?v=2.8.0"></script><script src="/js/stun-boot.js?v=2.8.0"></script><script src="/js/scroll.js?v=2.8.0"></script><script src="/js/header.js?v=2.8.0"></script><script src="/js/sidebar.js?v=2.8.0"></script></body></html>