<!DOCTYPE html><html lang="zh-HK"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.8.0" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.8.0" type="image/png" sizes="32x32"><meta name="description" content="K-means">
<meta property="og:type" content="article">
<meta property="og:title" content="CS3481-Lecture-5">
<meta property="og:url" content="https://lemonkai.github.io/2023/02/27/CS3481-Lecture-5/index.html">
<meta property="og:site_name" content="KAI Blog">
<meta property="og:description" content="K-means">
<meta property="og:locale" content="zh_HK">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230306204416.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230313191523.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230313194703.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230313194915.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230313195403.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230313201835.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230313201942.png">
<meta property="og:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230313202058.png">
<meta property="article:published_time" content="2023-02-27T10:52:40.000Z">
<meta property="article:modified_time" content="2023-03-13T12:21:13.167Z">
<meta property="article:author" content="KAI">
<meta property="article:tag" content="CS3481">
<meta property="article:tag" content="Fundamental Data Science">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/LemonKAI/image/main/20230306204416.png"><title>CS3481-Lecture-5 | KAI Blog</title><link ref="canonical" href="https://lemonkai.github.io/2023/02/27/CS3481-Lecture-5/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.8.0"><link rel="stylesheet" href="css/custom.css"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":false,"scrollDownIcon":false},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"複製","copySuccess":"複製成功","copyError":"複製失敗"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 6.3.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner header-inner--height header-inner--bgcolor"><nav class="header-nav header-nav--sticky"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首頁</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">歸檔</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">分類</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">標籤</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">CS3481-Lecture-5</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">發表於</span><span class="post-meta-item__value">2023-02-27</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新於</span><span class="post-meta-item__value">2023-03-13</span></span></div></header><div class="post-body">
        <h1 id="K-means"   >
          <a href="#K-means" class="heading-link"><i class="fas fa-link"></i></a><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h1>
      <span id="more"></span>


        <h1 id="Cluster-analysis"   >
          <a href="#Cluster-analysis" class="heading-link"><i class="fas fa-link"></i></a><a href="#Cluster-analysis" class="headerlink" title="Cluster analysis"></a>Cluster analysis</h1>
      <ul>
<li><p>Cluster analysis groups data objects based only on the attributes of the data</p>
</li>
<li><p>The main objective is that</p>
<ul>
<li>The objects within a group be similar to one another and</li>
<li>They are different from the objects in the other groups</li>
</ul>
</li>
<li><p>Cluster analysis is important in the following areas:</p>
<ul>
<li>Biology</li>
<li>Medicine</li>
<li>Information retrieval</li>
<li>Business</li>
</ul>
</li>
<li><p>Cluster analysis provides an abstraction form individual objects to the clusters in which those data objects reside</p>
</li>
<li><p>Some clustering techniques characterize each cluster in terms of a cluster prototype</p>
</li>
<li><p>The prototype is a data object that is representative of the other objects in the cluster</p>
</li>
</ul>

        <h1 id="Differet-type-of-clusterings"   >
          <a href="#Differet-type-of-clusterings" class="heading-link"><i class="fas fa-link"></i></a><a href="#Differet-type-of-clusterings" class="headerlink" title="Differet type of clusterings"></a>Differet type of clusterings</h1>
      <ul>
<li><p>We consider the following types of clusterings</p>
<ul>
<li>Partitional versus hierarchical</li>
<li>Exclusive versus fuzzy</li>
<li>Complete versus partial</li>
</ul>
</li>
</ul>

        <h2 id="Partitional-versus-hierachical"   >
          <a href="#Partitional-versus-hierachical" class="heading-link"><i class="fas fa-link"></i></a><a href="#Partitional-versus-hierachical" class="headerlink" title="Partitional versus hierachical"></a>Partitional versus hierachical</h2>
      <ul>
<li><p>A partitional clustering is a division of the set of data objects into subsets</p>
</li>
<li><p>A hierachical clustering is a set of nested clusters that are organized as a tree</p>
<ul>
<li>Each node (cluster) in the tree (except for the leaf nodes) is the union of its children (sub-clusters).</li>
<li>The root of the tree is the cluster containing all the objects.</li>
<li>Often, but not always, the leaves of the tree are singleton clusters of individual data objects.</li>
</ul>
</li>
<li><p>The following figures form a hierachical (nested) clustering with 1, 2, 4 and 6 clusters at each level.</p>
</li>
<li><p>A hierarchical clustering can be viewed as a sequence of partitional clusterings</p>
</li>
<li><p>A partitional clustering can be obtained by taking any member of that sequence, i.e. by cutting the hierarchical tree at a certain level.<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230306204416.png"></p>
</li>
</ul>

        <h2 id="Exclusive-versus-fuzzy"   >
          <a href="#Exclusive-versus-fuzzy" class="heading-link"><i class="fas fa-link"></i></a><a href="#Exclusive-versus-fuzzy" class="headerlink" title="Exclusive versus fuzzy"></a>Exclusive versus fuzzy</h2>
      <ul>
<li>In an exclusive clustering, each object is assigned to a single cluster</li>
<li>However, there are many situations in which a point could resonably be placed in more than one cluster</li>
<li>In a fuzzy clustering, every object belongs to every cluster with a membership wight that is between<ul>
<li>0 (absolutely does not belong) and</li>
<li>1 (absolutely belongs)</li>
</ul>
</li>
<li>This approach is useful for avoiding the arbitrariness of assigning an object to only one cluster when it is close to serveral.</li>
<li>A fuzzy clustering can be converted to an exclusive clustering by assigning each object to the cluster in which its membership value is the highest.</li>
</ul>

        <h2 id="Complete-versus-partial"   >
          <a href="#Complete-versus-partial" class="heading-link"><i class="fas fa-link"></i></a><a href="#Complete-versus-partial" class="headerlink" title="Complete versus partial"></a>Complete versus partial</h2>
      <ul>
<li>A complete clustering assigns every object to a cluster</li>
<li>A partial clustering does not assign every object to a cluster</li>
<li>The motivation of partial clustering is that some objects in a data set may not belong to well-defined groups.</li>
<li>Instead, they may represent noise or outliers</li>
</ul>

        <h1 id="K-means-1"   >
          <a href="#K-means-1" class="heading-link"><i class="fas fa-link"></i></a><a href="#K-means-1" class="headerlink" title="K-means"></a>K-means</h1>
      <ul>
<li><p>K-means is a prototype-based clustering techique which creates a one-level partitioning which creates a one-level partitioning of the data objects.</p>
</li>
<li><p>Specifically, K-means ddefines a prototype in terms of the centroid of a group of points</p>
</li>
<li><p>K-means is typically applied to objects in an n-dimensional space.</p>
</li>
<li><p>The basic K-means algorithm is summarized below</p>
<ol>
<li>Select K points as initial centroids</li>
<li>Repeat<br>  a. Form K clusters by assigning each point to its closest centroid.<br>  b. Re-compute the centroid of each cluster</li>
<li>Until centroids do not change</li>
</ol>
</li>
<li><p>We first choose K initial centroids, where K is a user-defined parameter, namely, the number of clusters desired.</p>
</li>
<li><p>Each point is then assigned to the closest centroid.</p>
</li>
<li><p>Each collection of points assigned to a centroid is a cluster</p>
</li>
<li><p>The centroid of each cluster is then updated based on the points assigned to the cluster</p>
</li>
<li><p>We repeat the assignment and update steps until the centroids remain the same.</p>
</li>
<li><p>These steps are illustrated in the following figures.</p>
</li>
<li><p>Starting from three centroids, the final  clusters are found in four assignment-update steps.</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230313191523.png"></p>
<ul>
<li><p>Each sub-figure shows</p>
<ul>
<li>The centroids at the start the iteration and</li>
<li>The assignment of the points to those centroids</li>
</ul>
</li>
<li><p>The centroids are indicated by the “+” symbol.</p>
</li>
<li><p>All points belonging to the same cluster have the same marker shape.</p>
</li>
<li><p>In the second step</p>
<ul>
<li>Points are assigned to the updated centroids and</li>
<li>The centroids are updated again</li>
</ul>
</li>
<li><p>We can observe that two of the centroids move to the two small groups of points at the bottom of the figures.</p>
</li>
<li><p>When the K-means algorithm terminates, the centroids hae identified the natural groupings of points</p>
</li>
</ul>

        <h2 id="Distance-measure"   >
          <a href="#Distance-measure" class="heading-link"><i class="fas fa-link"></i></a><a href="#Distance-measure" class="headerlink" title="Distance measure"></a>Distance measure</h2>
      <ul>
<li><p>To assign a point to the closest centroid, we need a measure that quantifies the notion of “closest”</p>
</li>
<li><p>Euclidean (L2) distance is often used for this purpose</p>
</li>
<li><p>The goal of clustering is typically expressed by an objective function</p>
</li>
<li><p>We consider the case where Euclidean distance is used</p>
</li>
<li><p>For our objective function, which measures the quality of a clustering solution, we can use the sum of the squared error (SSE)</p>
</li>
<li><p>We calculate the Euclidean distance of each data point to its associated centroid</p>
</li>
<li><p>We then compute the total sum of the squared distances, which is also known as the sum of the squared error(SSE)</p>
</li>
<li><p>A small value of SSE means that the prototpyes(centroids) of this clustering are good representations of the points in their clusters.</p>
</li>
<li><p>The SSE is defined as follows:</p>
<ul>
<li><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230313194703.png"></li>
</ul>
</li>
<li><p>In this equation</p>
<ul>
<li>x is a data object</li>
<li>Ci is the i-th cluster</li>
<li>ci is the centroid of cluster Ci</li>
<li>d is the Euclidean (L2) distance between two objects</li>
</ul>
</li>
<li><p>It can be shown that the mean of the data points in the cluster minimizes the SSE of the cluster</p>
</li>
<li><p>The centroid (mean) of the i-th cluster is defined as<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230313194915.png"></p>
</li>
<li><p>In this equation, mi is the number of objects in the i-th cluster</p>
</li>
<li><p>Step 2a adn 2b of the K-means algorithm attempt to minimize the SSE</p>
</li>
<li><p>Step 2a forms clusters by assigning each point to its nearest centroid, which minimizes the SSE for the given set of centroids</p>
</li>
<li><p>Step 2b recomputes the centroids so as to further minimize the SSE</p>
</li>
</ul>

        <h2 id="Choosing-initial-centroids"   >
          <a href="#Choosing-initial-centroids" class="heading-link"><i class="fas fa-link"></i></a><a href="#Choosing-initial-centroids" class="headerlink" title="Choosing initial centroids"></a>Choosing initial centroids</h2>
      <ul>
<li><p>Choosing the proper initial centroids is the key step of the basic K-means procedure</p>
</li>
<li><p>A common approach is to choose the initial centroids randomly</p>
</li>
<li><p>However, randomly selected initial centroids may be poor choices</p>
</li>
<li><p>This is illustrated in the following figures<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230313195403.png"></p>
</li>
<li><p>One technique that is commonly used to address the problem of choosing initial centroids is to perform multiple runs</p>
</li>
<li><p>Each run uses a different set of initial centroids</p>
</li>
<li><p>We then choose the set of clusters with the minimum SSE</p>
</li>
</ul>

        <h2 id="Outliers"   >
          <a href="#Outliers" class="heading-link"><i class="fas fa-link"></i></a><a href="#Outliers" class="headerlink" title="Outliers"></a>Outliers</h2>
      <ul>
<li><p>Outliers can influence the clusters that are found</p>
</li>
<li><p>When outliers are present, the resulting cluster centroids may not be as representative as they otherwise would be</p>
</li>
<li><p>Because of this, it is often useful to discover outliers and eliminate them beforehand.</p>
</li>
<li><p>To identify the outlers, we can keep track of the contribution of each point to the SSE</p>
</li>
<li><p>We then eliminate those points with unusaully high contributions to the SSE</p>
</li>
</ul>

        <h2 id="Post-processing"   >
          <a href="#Post-processing" class="heading-link"><i class="fas fa-link"></i></a><a href="#Post-processing" class="headerlink" title="Post-processing"></a>Post-processing</h2>
      <ul>
<li><p>Two post-porcessing straegies that decrease the SSE by increasing the number of clusters are </p>
<ul>
<li>Split a cluster<ul>
<li>The cluster which, when split into two sub-clusters, results in the largest decrease in SSE after updating the centroids is chosen</li>
</ul>
</li>
<li>Introduce a new cluster centroid<ul>
<li>Often the point that is farthest from its associated cluster centroid is chosen</li>
<li>We can determine this if we keep track of the contribution of each point to the SSE</li>
</ul>
</li>
</ul>
</li>
<li><p>Two post-processing strategies that decrease the number of clusters, while trying to minimize the increase in SSE, are </p>
<ul>
<li>Disprerse a cluster<ul>
<li>This is accomplished by removing the centroid that coreespoinds to the cluster</li>
<li>The points in that cluster are then re-assigned to other clusters.</li>
<li>The cluster that is dispersed should be the one that increases the SSE the least after updating the centroids</li>
</ul>
</li>
<li>Merge two clusters<ul>
<li>We can merge the two clusters that result in the smallest increase in SSE after updating the centroids</li>
</ul>
</li>
</ul>
</li>
</ul>

        <h2 id="Limitations-of-K-means"   >
          <a href="#Limitations-of-K-means" class="heading-link"><i class="fas fa-link"></i></a><a href="#Limitations-of-K-means" class="headerlink" title="Limitations of K-means"></a>Limitations of K-means</h2>
      <ul>
<li><p>K-means has a number of limitaions</p>
</li>
<li><p>In particular, the algorithm has difficulty in detecting clusters with non-spherical shapes or widely different sizes or densities</p>
</li>
<li><p>This is because K-means is designed to look for globular clusters of similar sizes and densities, or clusters that are well separated</p>
</li>
<li><p>This is illustrated by the following examples</p>
</li>
<li><p>In this example, K-means cannot find the three natural clusters because one of the clsuters is much larger than the other two.</p>
</li>
<li><p>As a result, the largest cluster is divided into sub-clusters</p>
</li>
<li><p>At the same time, one of the smaller clusters is combined with a portion of the largest cluster<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230313201835.png"></p>
</li>
<li><p>In this example, K-means fails to find the three natrual clsuters.</p>
</li>
<li><p>This is because the two smaller clusters are much denser than the largest cluster<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230313201942.png"></p>
</li>
<li><p>In this example, K-means finds two clusters that mix portions of the two natural clsuters</p>
</li>
<li><p>This is because the natural clusters are not globular in shape<br><img src="https://raw.githubusercontent.com/LemonKAI/image/main/20230313202058.png"></p>
</li>
</ul>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文結束，感謝您的閱讀 ------</div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://lemonkai.github.io/tags/CS3481/">CS3481</a></span><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://lemonkai.github.io/tags/Fundamental-Data-Science/">Fundamental Data Science</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2023/03/02/CS3103-Lecture-6/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">CS3103-Lecture-6</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2023/02/16/CS3103-Lecture-5/"><span class="paginator-prev__text">CS3103-Lecture-5</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div><div class="comments" id="comments"><div id="utterances-container"></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目錄</span><span class="sidebar-nav-ov">本站概覽</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#K-means"><span class="toc-number">1.</span> <span class="toc-text">
          K-means</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Cluster-analysis"><span class="toc-number">2.</span> <span class="toc-text">
          Cluster analysis</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Differet-type-of-clusterings"><span class="toc-number">3.</span> <span class="toc-text">
          Differet type of clusterings</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Partitional-versus-hierachical"><span class="toc-number">3.1.</span> <span class="toc-text">
          Partitional versus hierachical</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Exclusive-versus-fuzzy"><span class="toc-number">3.2.</span> <span class="toc-text">
          Exclusive versus fuzzy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Complete-versus-partial"><span class="toc-number">3.3.</span> <span class="toc-text">
          Complete versus partial</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#K-means-1"><span class="toc-number">4.</span> <span class="toc-text">
          K-means</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Distance-measure"><span class="toc-number">4.1.</span> <span class="toc-text">
          Distance measure</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Choosing-initial-centroids"><span class="toc-number">4.2.</span> <span class="toc-text">
          Choosing initial centroids</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Outliers"><span class="toc-number">4.3.</span> <span class="toc-text">
          Outliers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Post-processing"><span class="toc-number">4.4.</span> <span class="toc-text">
          Post-processing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Limitations-of-K-means"><span class="toc-number">4.5.</span> <span class="toc-text">
          Limitations of K-means</span></a></li></ol></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/stun-logo.svg" alt="avatar"></div><p class="sidebar-ov-author__text">Seize the Day</p></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">16</div><div class="sidebar-ov-state-item__name">歸檔</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">7</div><div class="sidebar-ov-state-item__name">分類</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">8</div><div class="sidebar-ov-state-item__name">標籤</div></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">你已閱讀了 </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2023</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>KAI</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script>function loadUtterances() {
  var d = document, s = d.createElement('script');
  var container = d.getElementById('utterances-container');

  if (!container) {
    return;
  }
  s.src = 'https://utteranc.es/client.js';
  s.setAttribute('repo', 'LemonKAI/KAIBlogComment');
  s.setAttribute('issue-term', 'title');
  s.setAttribute('label', 'utterances');
  s.setAttribute('theme', 'github-light');
  s.setAttribute('crossorigin', 'anonymous');
  s.setAttribute('async', '');
  if (false) {
    s.setAttribute('data-pjax-rm', '');
  }
  container.append(s);
}

if (false) {
  loadUtterances();
} else {
  window.addEventListener('DOMContentLoaded', loadUtterances, false);
}</script><script src="/js/utils.js?v=2.8.0"></script><script src="/js/stun-boot.js?v=2.8.0"></script><script src="/js/scroll.js?v=2.8.0"></script><script src="/js/header.js?v=2.8.0"></script><script src="/js/sidebar.js?v=2.8.0"></script></body></html>